#!/usr/bin/env python3
"""
Task 5 - Step 3/9: Data preprocessing (simplified)

Preprocess the dataset generated by 01_extract_data.py.
"""

import sys
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Locate input file
DATA_FILES = ["main_data.csv", "oncology_drugs_complete.csv",
              "oncology_drugs_data.csv", "epcoritamab_data.csv"]
DATA_FILE = None

for f in DATA_FILES:
    if os.path.exists(f):
        DATA_FILE = f
        break

if DATA_FILE is None:
    print("âŒ Error: data file not found")
    print()
    print("Please run: python 01_extract_data.py")
    sys.exit(1)

print(f"âœ… Input file: {DATA_FILE}")
if DATA_FILE == "main_data.csv":
    print("   (Task 5 dataset - 35 oncology drugs)")
elif DATA_FILE == "oncology_drugs_complete.csv":
    print("   (Complete oncology dataset)")
elif DATA_FILE == "oncology_drugs_data.csv":
    print("   (Multi-drug oncology dataset)")
else:
    print("   (Single-drug dataset)")
print()

# Load data
print("ğŸ“‚ Loading source data...")
df = pd.read_csv(DATA_FILE)
print(f"âœ… Raw data: {len(df)} rows, {len(df.columns)} columns")
print()

# Deduplicate on safetyreportid
if 'safetyreportid' in df.columns:
    before_dedup = len(df)
    df = df.drop_duplicates(subset='safetyreportid', keep='first')
    after_dedup = len(df)
    duplicates_removed = before_dedup - after_dedup
    
    if duplicates_removed > 0:
        print(f"âš ï¸  Deduplication removed {duplicates_removed} duplicate rows ({duplicates_removed/before_dedup*100:.2f}%)")
        print(f"   Remaining unique reports: {after_dedup}")
    else:
        print("âœ… Deduplication check: no duplicates found")
    print()

# Preprocessing pipeline
print("=" * 80)
print("ğŸ”„ Running preprocessing pipeline")
print("=" * 80)
print()

print("Processing steps:")
print("  1ï¸âƒ£  Convert numeric fields")
print("  2ï¸âƒ£  Feature engineering (create new features)")
print("  3ï¸âƒ£  Handle missing values")
print("  4ï¸âƒ£  Clean data")
print()

# Step 1: convert numeric fields
print("1ï¸âƒ£  Converting numeric fields...")

# Convert severity indicators to numeric
severity_cols = ['serious', 'seriousnessdeath', 'seriousnesshospitalization',
                 'seriousnesslifethreatening', 'seriousnessdisabling',
                 'seriousnesscongenitalanomali', 'seriousnessother']

for col in severity_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        # Convert to binary: treat any non-zero value as 1
        df[col] = (df[col] > 0).astype(int)

# Convert patient attributes
if 'patientsex' in df.columns:
    df['patientsex'] = pd.to_numeric(df['patientsex'], errors='coerce').fillna(0).astype(int)

if 'patientonsetage' in df.columns:
    df['patientonsetage'] = pd.to_numeric(df['patientonsetage'], errors='coerce')

if 'patientweight' in df.columns:
    df['patientweight'] = pd.to_numeric(df['patientweight'], errors='coerce')

# Convert drug and reaction counts
if 'num_drugs' in df.columns:
    df['num_drugs'] = pd.to_numeric(df['num_drugs'], errors='coerce').fillna(1).astype(int)

if 'num_reactions' in df.columns:
    df['num_reactions'] = pd.to_numeric(df['num_reactions'], errors='coerce').fillna(1).astype(int)

print("âœ… Numeric conversion complete")
print()

original_cols_count = len(df.columns)

# Step 2: feature engineering
print("2ï¸âƒ£  Feature engineering...")

# Build age-related features and handle outliers
if 'patientonsetage' in df.columns:
    # Treat out-of-range ages (<0 or >120) as missing
    df.loc[df['patientonsetage'] > 120, 'patientonsetage'] = np.nan
    df.loc[df['patientonsetage'] < 0, 'patientonsetage'] = np.nan

    # Age buckets
    df['age_group'] = pd.cut(
        df['patientonsetage'],
        bins=[0, 18, 45, 65, 120],
        labels=['0-18', '19-45', '46-65', '66+'],
        include_lowest=True,
    )

    # One-hot encode the age buckets
    age_dummies = pd.get_dummies(df['age_group'], prefix='age')
    df = pd.concat([df, age_dummies], axis=1)

    # Flag missing age values
    df['age_missing'] = df['patientonsetage'].isna().astype(int)

# Build gender features
if 'patientsex' in df.columns:
    df['sex_male'] = (df['patientsex'] == 1).astype(int)
    df['sex_female'] = (df['patientsex'] == 2).astype(int)
    df['sex_unknown'] = (df['patientsex'] == 0).astype(int)

# Multi-drug usage features
if 'num_drugs' in df.columns:
    df['polypharmacy'] = (df['num_drugs'] > 1).astype(int)
    df['high_polypharmacy'] = (df['num_drugs'] > 5).astype(int)

# Reaction count features
if 'num_reactions' in df.columns:
    df['multiple_reactions'] = (df['num_reactions'] > 1).astype(int)
    df['many_reactions'] = (df['num_reactions'] > 3).astype(int)

# Reminder: do not recreate the severity_score feature (introduces leakage)
print("âœ… Feature engineering complete")
print(f"   New features added: {len(df.columns) - original_cols_count}")
print()

# Save snapshot of the preprocessed data
print("ğŸ’¾ Saving preprocessed data...")
df.to_csv("preprocessed_data.csv", index=False)
print("âœ… Saved: preprocessed_data.csv")
print()

# Display new feature names
print("ğŸ†• New feature list:")
original_cols = set(pd.read_csv(DATA_FILE).columns)
new_cols = [col for col in df.columns if col not in original_cols]
for i, col in enumerate(new_cols, 1):
    print(f"  {i:2d}. {col}")
print()

# Step 3: prepare training data
print("ğŸ¯ Preparing training data")
print("Target variable: seriousnessdeath (death indicator)")

y = pd.to_numeric(df['seriousnessdeath'], errors='coerce').fillna(0).astype(int)
positive_count = int(y.sum())
negative_count = int(len(y) - positive_count)

# Target label overview
print("Target distribution:")
print(f"  Positive (death): {positive_count} ({positive_count/len(y)*100:.1f}%)")
print(f"  Negative (survival): {negative_count} ({negative_count/len(y)*100:.1f}%)")

# Select feature columns
print("ğŸ” Selecting features...")

# Fields to exclude from modelling
exclude_cols = [
    'safetyreportid',
    'receivedate',  # raw date string
    'target_drug',  # drug name (text)
    'drugname',  # alternate drug name (text)
    'all_drugs',  # list of drugs (text)
    'drug_indication',  # indication text
    'reactions',  # reaction list
    'patientonsetageunit',  # original age unit
    'age_group',  # bucket already expanded via one-hot encoding
    'reporter_qualification',  # reporter qualification (text)
    # Target labels and related severity flags
    'serious',
    'seriousnessdeath',
    'seriousnesshospitalization',
    'seriousnesslifethreatening',
    'seriousnessdisabling',
    'seriousnesscongenitalanomali',
    'seriousnessother',
]

# Filter numeric features using robust dtype checks
feature_cols = [
    col for col in df.columns
    if col not in exclude_cols and np.issubdtype(df[col].dtype, np.number)
]

X = df[feature_cols].copy()
class_counts = y.value_counts()
min_class_count = int(class_counts.min())

# âœ… Safety check: ensure no leakage columns are present
leakage_cols = [col for col in feature_cols if col.startswith('seriousness') or col == 'serious']
if leakage_cols:
    raise AssertionError(f"âŒ Data leakage! Exclude columns: {leakage_cols}")
print("âœ… Leakage check passed: no severity-related columns present")
print(f"âœ… Selected {len(feature_cols)} features")
print("Feature columns:")
for i, col in enumerate(feature_cols, 1):
    print(f"  {i:2d}. {col}")
print()

# Step 4: handle missing values
print("ğŸ”§ Handling missing values...")
print("   Missing value summary:")
missing_counts = X.isnull().sum()
missing_cols = missing_counts[missing_counts > 0]
if len(missing_cols) > 0:
    for col, count in missing_cols.items():
        pct = count / len(X) * 100
        print(f"     {col}: {count} ({pct:.1f}%)")
else:
    print("     None")

# Impute with median
imputer = SimpleImputer(strategy='median')
X_imputed = pd.DataFrame(
    imputer.fit_transform(X),
    columns=X.columns,
    index=X.index
)
print("âœ… Missing value handling complete")
print()

# Step 5: split train/test sets
print("ğŸ“Š Splitting train and test sets...")

# âœ… Fallback if stratified split is not feasible
if min_class_count >= 2:
    # Stratified split when both classes have >=2 samples
    X_train, X_test, y_train, y_test = train_test_split(
        X_imputed, y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )
    print("Using stratified split (stratify=y)")
else:
    # Class counts too small, fallback to unstratified split
    print(f"âš ï¸  Warning: minimum class size {min_class_count}; using non-stratified split")
    X_train, X_test, y_train, y_test = train_test_split(
        X_imputed, y,
        test_size=0.2,
        random_state=42,
        stratify=None,
    )

print(f"Train set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")

# âœ… Safety check: ensure no shared IDs between train/test
if 'safetyreportid' in df.columns:
    ids_train = set(df.loc[X_train.index, 'safetyreportid'])
    ids_test = set(df.loc[X_test.index, 'safetyreportid'])
    inter = ids_train & ids_test
    if len(inter) > 0:
        print(f"âŒ Warning: ID leakage between train/test! Example IDs: {list(inter)[:5]}")
    else:
        print("âœ… ID leakage check passed: no shared IDs")

print("Train target distribution:")
train_pos = int(y_train.sum())
train_neg = int(len(y_train) - train_pos)
print(f"  Positive: {train_pos} ({train_pos/len(y_train)*100:.1f}%)")
print(f"  Negative: {train_neg} ({train_neg/len(y_train)*100:.1f}%)")

print("Test target distribution:")
test_pos = int(y_test.sum())
test_neg = int(len(y_test) - test_pos)
print(f"  Positive: {test_pos} ({test_pos/len(y_test)*100:.1f}%)")
print(f"  Negative: {test_neg} ({test_neg/len(y_test)*100:.1f}%)")

# Save datasets
print("ğŸ’¾ Saving train and test datasets...")
X_train.to_csv("X_train.csv", index=False)
y_train.to_csv("y_train.csv", index=False, header=['seriousnessdeath'])
X_test.to_csv("X_test.csv", index=False)
y_test.to_csv("y_test.csv", index=False, header=['seriousnessdeath'])

print("âœ… Saved:")
print("  - X_train.csv")
print("  - y_train.csv")
print("  - X_test.csv")
print("  - y_test.csv")

# âœ… Save preprocessing metadata
import json
import time

preprocess_meta = {
    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    "input_records": len(df),
    "output_records": len(df),
    "duplicates_removed": 0,  # handled during deduplication
    "target_variable": "seriousnessdeath",
    "positive_count": int(y.sum()),
    "negative_count": int(len(y) - y.sum()),
    "positive_rate": float(y.mean()),
    "negative_rate": float(1 - y.mean()),
    "train_size": len(X_train),
    "test_size": len(X_test),
    "train_pos": train_pos,
    "train_neg": train_neg,
    "test_pos": test_pos,
    "test_neg": test_neg,
    "num_features": len(feature_cols),
    "feature_cols": feature_cols,
    "excluded_cols": exclude_cols,
    "random_state": 42,
    "stratified": min_class_count >= 2
}

with open("preprocess_meta.json", "w", encoding='utf-8') as f:
    json.dump(preprocess_meta, f, indent=2, ensure_ascii=False)

print("  - preprocess_meta.json")
print()

# Show sample rows
print("=" * 80)
print("ğŸ“‹ Sample of preprocessed data")
print("=" * 80)
print()
print(df[['target_drug', 'seriousnessdeath', 'patientonsetage', 'patientsex', 
          'num_drugs', 'num_reactions']].head(3))
print()

# Summary
print("=" * 80)
print("âœ… Step 3 complete - preprocessing finished")
print("=" * 80)
print()

print("ğŸ“ Generated files:")
print("  1. preprocessed_data.csv - preprocessed dataset")
print("  2. X_train.csv - training features")
print("  3. y_train.csv - training labels")
print("  4. X_test.csv - test features")
print("  5. y_test.csv - test labels")
print()

print("ğŸ¯ Next steps:")
print("  Run: python 04_train_models.py")
print("  Purpose: train multiple machine learning models")
print()

